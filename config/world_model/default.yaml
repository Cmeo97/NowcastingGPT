# _target_: models.TransformerConfig
# tokens_per_block: 64
# max_blocks: 9 # length of the sequence
# attention: 'causal'
# num_layers: 8
# num_heads: 4
# embed_dim: 64
# embed_pdrop: 0.1
# resid_pdrop: 0.1
# attn_pdrop: 0.1

_target_: models.TransformerConfig
tokens_per_block: 64
max_blocks: 9 # length of the sequence
attention: 'causal'
num_layers: 12
num_heads: 8
embed_dim: 128
embed_pdrop: 0.1
resid_pdrop: 0.1
attn_pdrop: 0.1